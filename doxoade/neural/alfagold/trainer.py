# doxoade/neural/alfagold/trainer.py
import time
import numpy as np
import os
from colorama import Fore, Style

from .model import Alfagold
from .optimizer import AdamW
from ..adapter import BrainLoader 

class AlfaTrainer:
    def __init__(self, model_path=None):
        self.model_path = model_path or os.path.expanduser("~/.doxoade/alfagold_v1.pkl")
        
        # Inicializa Modelo
        self.model = Alfagold(vocab_size=2000, d_model=64, max_len=128)
        
        if os.path.exists(self.model_path):
            try:
                self.model.load(self.model_path)
                print(Fore.GREEN + "   ðŸ’¾ Modelo carregado.")
            except: pass
            
        self.optimizer = AdamW(self.model.params, lr=0.001)
        self.loader = BrainLoader()

    def train_cycle(self, epochs=10, samples=500, difficulty=1):
        print(Fore.YELLOW + f"   ðŸš€ Iniciando Treino Alfagold (NÃ­vel {difficulty})...")
        
        # 1. Gerar Dados
        raw_data = self.loader.get_training_data(limit=samples, difficulty=difficulty)
        
        # 2. Treinar Tokenizer
        full_text = " ".join([p[0] + " " + p[1] for p in raw_data])
        print(f"   ðŸ”¨ Refinando BPE com {len(full_text)} caracteres...")
        self.model.tokenizer.train(full_text, vocab_size=2000)
        
        start_time = time.time()
        
        for epoch in range(epochs):
            total_loss = 0
            count = 0
            
            np.random.shuffle(raw_data)
            
            for input_str, target_str in raw_data:
                full_seq = input_str + " " + target_str
                ids = self.model.tokenizer.encode(full_seq)
                
                if len(ids) < 2: continue
                
                x_ids = ids[:-1]
                y_ids = np.array(ids[1:])
                
                # --- CICLO DE TREINO EXPLÃCITO ---
                
                # A. Forward
                logits, cache = self.model.forward(x_ids)
                
                # B. Loss (Cross Entropy Manual)
                N = logits.shape[0]
                # Estabilidade numÃ©rica do Softmax
                exps = np.exp(logits - np.max(logits, axis=-1, keepdims=True))
                probs = exps / np.sum(exps, axis=-1, keepdims=True)
                
                correct_probs = probs[np.arange(N), y_ids]
                loss = -np.sum(np.log(correct_probs + 1e-9)) / N
                
                # C. Gradiente da Loss
                dlogits = probs
                dlogits[np.arange(N), y_ids] -= 1
                dlogits /= N
                
                # D. Backward
                grads = self.model.backward(dlogits, cache)
                
                # E. Update
                self.optimizer.step(grads)
                
                total_loss += loss
                count += 1
            
            avg_loss = total_loss / count if count > 0 else 0
            
            # Reporta progresso
            if (epoch + 1) % 1 == 0: 
                elapsed = time.time() - start_time
                print(f"   Epoca {epoch+1}/{epochs}: Loss {avg_loss:.4f} ({elapsed:.1f}s)")
                start_time = time.time()
                
        self.model.save(self.model_path)
        print(Fore.GREEN + "   ðŸ’¾ Modelo salvo e atualizado.")