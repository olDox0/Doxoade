# lab_alfagold.py
import numpy as np
import time
import sys
import os

# Garante que conseguimos importar o pacote doxoade
sys.path.insert(0, os.path.abspath('.'))

from doxoade.neural.alfagold.model import Alfagold
from doxoade.neural.alfagold.attention import execute_attention, scaled_dot_product_attention, flash_attention_numpy
from colorama import init, Fore, Style

init(autoreset=True)

def test_mathematics():
    print(Fore.YELLOW + "\nüß™ [TESTE 1] Validando Matem√°tica da Aten√ß√£o...")
    
    # Configura√ß√£o
    BATCH = 1
    SEQ_LEN = 128
    D_MODEL = 64
    
    # Cria tensores aleat√≥rios simulando Q, K, V
    np.random.seed(42)
    Q = np.random.randn(BATCH, SEQ_LEN, D_MODEL).astype(np.float32)
    K = np.random.randn(BATCH, SEQ_LEN, D_MODEL).astype(np.float32)
    V = np.random.randn(BATCH, SEQ_LEN, D_MODEL).astype(np.float32)

    print(f"   Matrizes criadas: Shape {Q.shape}")

    # 1. Executa Aten√ß√£o Padr√£o (Exata)
    start = time.perf_counter()
    out_std, _ = scaled_dot_product_attention(Q, K, V)
    t_std = (time.perf_counter() - start) * 1000
    print(f"   üîπ Standard Attention: {t_std:.4f}ms")

    # 2. Executa Flash Attention (Aproximada/Blocada)
    start = time.perf_counter()
    out_flash, _ = flash_attention_numpy(Q, K, V, block_size=64)
    t_flash = (time.perf_counter() - start) * 1000
    print(f"   ‚ö° Flash Attention:    {t_flash:.4f}ms")

    # 3. Compara√ß√£o
    # Nota: Pequenas diferen√ßas s√£o esperadas devido a ponto flutuante e ordem de soma
    diff = np.mean(np.abs(out_std - out_flash))
    print(f"   üìâ Diferen√ßa M√©dia (Erro): {diff:.8f}")

    if diff < 0.1: 
        print(Fore.GREEN + "   ‚úÖ SUCESSO: As implementa√ß√µes s√£o matematicamente equivalentes.")
    else:
        print(Fore.RED + "   ‚ùå FALHA: Diverg√™ncia matem√°tica detectada.")

def test_pipeline():
    print(Fore.YELLOW + "\nüß™ [TESTE 2] Pipeline Completo do Modelo...")
    
    model = Alfagold(vocab_size=100, d_model=32)
    
    # Simula um treino r√°pido de vocabul√°rio
    corpus = "def salvar_arquivo(nome): with open(nome, 'w') as f: f.write('teste')"
    model.train_tokenizer(corpus)
    
    # Predi√ß√£o
    texto_teste = "def salvar"
    try:
        output = model.predict(texto_teste)
        
        # Verifica shape de sa√≠da
        # Esperado: (1, Num_Tokens, D_Model) -> O 1 √© batch size impl√≠cito no nosso c√≥digo simples
        expected_tokens = len(model.tokenizer.encode(texto_teste))
        expected_dim = 32
        
        print(f"   Shape de Sa√≠da: {output.shape}")
        
        if output.shape[-1] == expected_dim:
            print(Fore.GREEN + "   ‚úÖ SUCESSO: O fluxo de tensores est√° correto.")
        else:
            print(Fore.RED + f"   ‚ùå FALHA: Dimens√£o incorreta. Esperado {expected_dim}, recebido {output.shape[-1]}")
            
    except Exception as e:
        print(Fore.RED + f"   ‚ùå CRASH: {e}")
        import traceback
        traceback.print_exc()

def test_stress():
    print(Fore.YELLOW + "\nüß™ [TESTE 3] Stress Test (Contexto Longo)...")
    # Tenta disparar o gatilho de 2048 tokens do attention.py
    
    LONG_SEQ = 2100
    D_MODEL = 32
    
    print(f"   Gerando sequ√™ncia massiva de {LONG_SEQ} tokens...")
    Q = np.random.randn(1, LONG_SEQ, D_MODEL).astype(np.float32)
    
    start = time.perf_counter()
    # Chama a fun√ß√£o mestra que deve decidir usar Flash
    out, _ = execute_attention(Q, Q, Q)
    duration = (time.perf_counter() - start) 
    
    print(Fore.GREEN + f"   ‚úÖ SUCESSO: Processou {LONG_SEQ} tokens em {duration:.4f}s sem estourar mem√≥ria.")

if __name__ == "__main__":
    print(Style.BRIGHT + "üî¨ INICIANDO BATERIA DE TESTES ALFAGOLD")
    test_mathematics()
    test_pipeline()
    test_stress()
    print(Style.BRIGHT + "\nüèÅ FIM DOS TESTES")